\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Xiong2017EndtoEndNA}
\citation{Yang2017AnseriniET}
\citation{Beltagy2020LongformerTL}
\citation{Beltagy2020LongformerTL}
\citation{Guo2016ADR}
\citation{Hui2017PACRRAP,Hui2018CoPACRRAC}
\citation{Xiong2017EndtoEndNA,Xiong2017ConvolutionalNN}
\citation{Vaswani2017AttentionIA}
\citation{Devlin2019BERTPO}
\citation{MacAvaney2019CEDRCE}
\citation{nogueira_passage_2020,yang_simple_2019}
\citation{Li2020PARADEPR}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\newlabel{related-work}{{2}{1}{Related Work}{section.2}{}}
\citation{Beltagy2020LongformerTL}
\citation{Craswell2020OverviewOT}
\citation{Craswell2020OverviewOT}
\citation{Chen2019UCASAT}
\citation{Beltagy2020LongformerTL}
\citation{Kovaleva2019RevealingTD}
\citation{Vaswani2017AttentionIA}
\citation{Campos2016MSMA}
\citation{Yang2018AnseriniRR}
\citation{Yang2017AnseriniET}
\citation{10.1145/3239571}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The attention patterns of the LongFormer model. In our experiments, we globally attend to the query tokens. Image credit: "Longformer: The Long-Document Transformer"\relax }}{2}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:attention}{{1}{2}{The attention patterns of the LongFormer model. In our experiments, we globally attend to the query tokens. Image credit: "Longformer: The Long-Document Transformer"\relax }{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Document length statistics from a representative sample ($n=1000$) of the dataset\relax }}{2}{table.caption.4}\protected@file@percent }
\newlabel{doc-length}{{1}{2}{Document length statistics from a representative sample ($n=1000$) of the dataset\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Longformer architecture}{2}{section.3}\protected@file@percent }
\newlabel{background}{{3}{2}{Longformer architecture}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Dataset}{2}{section.4}\protected@file@percent }
\newlabel{dataset}{{4}{2}{Dataset}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Baseline}{2}{section.5}\protected@file@percent }
\newlabel{baseline}{{5}{2}{Baseline}{section.5}{}}
\citation{Craswell2020OverviewOT}
\citation{Devlin2019BERTPO}
\citation{Liu2019RoBERTaAR}
\citation{Kingma2015AdamAM}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of statistics of TREC 2019 DLT challenge. The statistics are reproduced here verbatim \citep  {Craswell2020OverviewOT}\relax }}{3}{table.caption.5}\protected@file@percent }
\newlabel{datastats}{{2}{3}{Summary of statistics of TREC 2019 DLT challenge. The statistics are reproduced here verbatim \cite {Craswell2020OverviewOT}\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Summary of baseline results on the TREC DLT 2019 corpus using a fine-tuned BM25 model (with and without relevance feedback).\relax }}{3}{table.caption.6}\protected@file@percent }
\newlabel{baseline-stats}{{3}{3}{Summary of baseline results on the TREC DLT 2019 corpus using a fine-tuned BM25 model (with and without relevance feedback).\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Evaluation}{3}{section.6}\protected@file@percent }
\newlabel{evaluation}{{6}{3}{Evaluation}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experimental Setup}{3}{section.7}\protected@file@percent }
\newlabel{experiments}{{7}{3}{Experimental Setup}{section.7}{}}
\bibstyle{ACM-Reference-Format}
\bibdata{milestone}
\bibcite{Beltagy2020LongformerTL}{{1}{2020}{{Beltagy et~al\mbox  {.}}}{{Beltagy, Peters, and Cohan}}}
\bibcite{Campos2016MSMA}{{2}{2016}{{Campos et~al\mbox  {.}}}{{Campos, Nguyen, Rosenberg, Song, Gao, Tiwary, Majumder, Deng, and Mitra}}}
\bibcite{Chen2019UCASAT}{{3}{2019}{{Chen et~al\mbox  {.}}}{{Chen, Li, He, and Sun}}}
\bibcite{Craswell2020OverviewOT}{{4}{2020}{{Craswell et~al\mbox  {.}}}{{Craswell, Mitra, Yilmaz, Campos, and Voorhees}}}
\bibcite{Devlin2019BERTPO}{{5}{2019}{{Devlin et~al\mbox  {.}}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{Guo2016ADR}{{6}{2016}{{Guo et~al\mbox  {.}}}{{Guo, Fan, Ai, and Croft}}}
\bibcite{Hui2017PACRRAP}{{7}{2017}{{Hui et~al\mbox  {.}}}{{Hui, Yates, Berberich, and Melo}}}
\bibcite{Hui2018CoPACRRAC}{{8}{2018}{{Hui et~al\mbox  {.}}}{{Hui, Yates, Berberich, and Melo}}}
\bibcite{Kingma2015AdamAM}{{9}{2015}{{Kingma and Ba}}{{Kingma and Ba}}}
\bibcite{Kovaleva2019RevealingTD}{{10}{2019}{{Kovaleva et~al\mbox  {.}}}{{Kovaleva, Romanov, Rogers, and Rumshisky}}}
\bibcite{Li2020PARADEPR}{{11}{2020}{{Li et~al\mbox  {.}}}{{Li, Yates, MacAvaney, He, and Sun}}}
\bibcite{Liu2019RoBERTaAR}{{12}{2019}{{Liu et~al\mbox  {.}}}{{Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}}}
\bibcite{MacAvaney2019CEDRCE}{{13}{2019}{{MacAvaney et~al\mbox  {.}}}{{MacAvaney, Yates, Cohan, and Goharian}}}
\bibcite{nogueira_passage_2020}{{14}{2020}{{Nogueira and Cho}}{{Nogueira and Cho}}}
\bibcite{Vaswani2017AttentionIA}{{15}{2017}{{Vaswani et~al\mbox  {.}}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{Xiong2017ConvolutionalNN}{{16}{2017a}{{Xiong et~al\mbox  {.}}}{{Xiong, Callan, and Liu}}}
\bibcite{Xiong2017EndtoEndNA}{{17}{2017b}{{Xiong et~al\mbox  {.}}}{{Xiong, Dai, Callan, Liu, and Power}}}
\bibcite{Yang2017AnseriniET}{{18}{2017}{{Yang et~al\mbox  {.}}}{{Yang, Fang, and Lin}}}
\bibcite{Yang2018AnseriniRR}{{19}{2018a}{{Yang et~al\mbox  {.}}}{{Yang, Fang, and Lin}}}
\bibcite{10.1145/3239571}{{20}{2018b}{{Yang et~al\mbox  {.}}}{{Yang, Fang, and Lin}}}
\bibcite{yang_simple_2019}{{21}{2019}{{Yang et~al\mbox  {.}}}{{Yang, Zhang, and Lin}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.185pt}
\newlabel{tocindent2}{0pt}
\newlabel{tocindent3}{0pt}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results \& Analysis}{4}{section.8}\protected@file@percent }
\newlabel{results}{{8}{4}{Results \& Analysis}{section.8}{}}
\@writefile{toc}{\contentsline {section}{References}{4}{section*.9}\protected@file@percent }
\newlabel{TotPages}{{4}{4}{}{page.4}{}}
